[
    {
        "pii": "S1574013725000681",
        "title": "Revolutionizing textual data insights: A comprehensive review of the dual relationship between transformers and clustering in textual data analysis",
        "abstract": "In recent years, the integration of transformer models and clustering techniques has gained significant attention in the research community. Transformers excel at feature extraction, representation learning, and understanding data, which helps improve the accuracy and efficiency of clustering tasks. Conversely, clustering methods play a critical role in managing data distribution, enhancing interpretability, and improving the training of transformer models. This review looks at the dual relationship between these two domains: how transformers can advance clustering methodologies and how clustering techniques can optimize transformer performance. By examining this interaction, the paper highlights promising directions for future research."
    },
    {
        "pii": "S0378779625006273",
        "title": "Wind farm transformer protection against lightning transients using air core reactor and resistor",
        "abstract": "Integrating renewable energy resources such as wind farms is an increasingly prominent trend for future power grids. However, the wind generator tower is consistently at risk of lightning strikes, putting the wind farms’ transformer at risk of damage by lightning transients traveling through the system. Various harmonic contents of the lightning transient can excite the transformer’s resonance frequencies, resulting in both terminal and internal overvoltages (OVs). To effectively safeguard transformers against resonance OVs, it is imperative to first identify the resonance points of the transformer. Following this, a protective method must be implemented to mitigate harmonic content magnitudes that contribute to resonance. This paper introduces a series-protection device comprising an air core reactor and suppressor resistance designed to protect the transformer. The research aims to provide solutions to safeguard wind farm transformers from both terminal and internal resonance OVs caused by lightning transients. The effectiveness of the protection device is assessed through analysis, simulation, and experiments conducted in a high-voltage laboratory setup."
    },
    {
        "pii": "S0893608025006495",
        "title": "PRformer: Pyramidal recurrent transformer for multivariate time series forecasting",
        "abstract": "The self-attention mechanism in Transformer architecture, invariant to sequence order, necessitates positional embeddings to encode temporal order in time series prediction. We argue that this reliance on positional embeddings restricts the Transformer’s ability to effectively represent temporal sequences, particularly when employing longer lookback windows. To address this, we introduce an innovative approach that combines Pyramid RNN embeddings (PRE) for univariate time series with the Transformer’s capability to model multivariate dependencies. PRE, utilizing pyramidal one-dimensional convolutional layers, constructs multiscale convolutional features that preserve temporal order. Additionally, RNNs, layered atop these features, learn multiscale time series representations sensitive to sequence order. This integration into Transformer models with attention mechanisms results in significant performance enhancements. We present the PRformer, a model integrating PRE with a standard Transformer encoder, demonstrating state-of-the-art performance on various real-world datasets. This performance highlights the effectiveness of our approach in leveraging longer lookback windows and underscores the critical role of robust temporal representations in maximizing Transformer’s potential for prediction tasks."
    },
    {
        "pii": "S2451904925006055",
        "title": "Thermal hazards and fire risk evaluation of transformer oil - heat transfer dynamics and safety implications",
        "abstract": "The rising demand for transformers, driven by increasing global electricity consumption and the growth of digital infrastructure, has led to widespread use of transformer oil as an insulating and cooling medium. However, the flammability of transformer oil introduces significant fire safety concerns in built environment. Furthermore, the interaction between transformer oil pool fires and nearby industrial processes, vegetation and others also presents a complex and potentially hazardous scenario for safety and infrastructure driven by radiative and convective heat transfer. Previous studies have suggested that transformer oil shares properties similar to diesel fuel, indicating its potential as an alternative fuel for Compression Ignition (CI) engines. This necessitates the establishment of storage tanks for safe storage and usage. Consequently, it is essential to assess the safety standards of transformer oil storage facilities and evaluate potential fire risks. Building on this, present study investigates the thermal behavior and fire risk potential of transformer oil under pool fire conditions. A series of controlled pool fire experiments with diameters ranging from 0.05 m to 0.5 m,are conducted analyzing critical parameters such as mass burning rate, flame geometry, and flame centerline temperature. These experimental results were benchmarked against those of conventional hydrocarbon fuels like gasoline and diesel. Advanced computational methods were employed, including MATLAB for determining flame geometry specifics (height, zones, and area) and Fire Dynamics Simulator (FDS) for modeling the 0.5 m diameter transformer oil pool fires. Due to the scarcity of experimental data on heat flux to the surroundings, a mathematical model was developed to estimate this parameter, which was then validated against FDS simulation results. The findings indicate that transformer oil has lower combustibility relative to gasoline and diesel, characterized by reduced burning rates, lower flame temperatures, and diminished radiation output (by 5%–20%). Radiative heat flux decreases with distance, and safety distances vary by fuel type. Transformer oil fires require 1.62–1.66 times smaller distances than diesel/gasoline but 1.46–1.66 times greater than biofuels, per NFPA 49 CFR (5 kW/m 2 for humans) and EN 1473 (1.5 kW/m 2 without protective clothing). Effect of nearby vegetation on substation and transformer oil is also discussed. The findings offer valuable insights for thermally-informed fire risk assessments, aiding in the development of safer design protocols and more resilient energy systems in industrial settings."
    },
    {
        "pii": "S030626192501013X",
        "title": "Optimizing software defined battery systems for transformer protection",
        "abstract": "Residential electric vehicle charging causes large spikes in electricity demand that risk violating neighborhood transformer power limits. Battery energy storage systems reduce these transformer limit violations, but operating them individually is not cost-optimal. Instead of individual optimization, aggregating, or sharing, these batteries leads to cost-optimal performance, but homeowners must relinquish battery control. This paper leverages virtualization to propose battery sharing optimization schemes to reduce electricity costs, extend the lifetime of a residential transformer, and maintain homeowner control over the battery. A case study with simulated home loads, solar generation, and electric vehicle charging profiles demonstrates that joint, or shared, optimization reduces consumer bills by 56 % and transformer aging by 48 % compared to individual optimization. Hybrid and dynamic optimization schemes that provide owners with autonomy have similar transformer aging reduction but are slightly less cost-effective. These results suggest that controlling shared batteries with virtualization is an effective way to delay transformer upgrades in the face of growing residential electric vehicle charging penetration."
    },
    {
        "pii": "S0952197625017932",
        "title": "An effective convolutional and transformer cooperation network for underwater acoustic target recognition",
        "abstract": "Underwater acoustic target recognition (UATR) is a key technology in the field of underwater acoustic information processing. In recent years, models based on convolutional neural networks (CNN) have shown excellent performance in the domain of UATR. However, CNN have limitations in capturing the global information of underwater acoustic features. Due to its advantages in modeling global dependencies, the Transformer model is gradually gaining attention from researchers. In order to capture the time-frequency dependencies in acoustic spectrograms more effectively, this paper proposes a recognition model based on the Mel spectrogram that combines CNN with the Transformer, named the underwater acoustics CNN-Transformer cooperation network (UACTC). Compared to the Transformer alone, this model is more efficient in extracting local features. The CNN module uses a residual network based on the efficient channel attention (ECA) module for efficient deep feature extraction. Additionally, the ECA module is introduced into the Transformer block to enhance the channel feature extraction of the Transformer. Experiments prove that the ECA module effectively improves the performance of the recognition system. The effectiveness of the proposed model has been validated on two public datasets, achieving 98.05 % and 96.96 % on the ShipsEar and DeepShip datasets, respectively."
    },
    {
        "pii": "S2772671125001305",
        "title": "Analyzing the influence of ZnO nanoparticles on transformer oil using dissolved gas analysis",
        "abstract": "Transformer oil-based nanofluids have attracted considerable attention in recent years due to their improved thermal and dielectric properties, which are essential for the efficient and reliable operation of power transformers. The addition of nanoparticles, such as Al₂O₃, SiO₂, TiO₂, and ZnO, plays a pivotal role in enhancing the heat dissipation and insulation characteristics of transformer oil. Despite this, research on the use of ZnO nanoparticles in transformer oil remains limited. This study focuses on the preparation and evaluation of transformer oil-based nanofluids with ZnO nanoparticles at varying weight percentages (0.01 %, 0.05 %, 0.075 %, and 0.1 %). Dissolved Gas Analysis (DGA) which identifies the types and concentrations of dissolved gas in transformer oil, can reveal details on fault indicators in power transformers. The diagnostic methods applied included the Doernenburg Ratio, Roger's Ratio, and the IEC method. In this study, we used the interpretation of the IEEE std 2008-C57.104 and IEC 2015-60599. The results of the DGA test are used to assess the conditions and identify potential disturbances in the power transformer. The outcome of the fault indication is also influenced by the different gas analysis techniques employed.The findings indicate that incorporating ZnO nanoparticles significantly alters the dissolved gas patterns, with each diagnostic method offering unique perspectives on fault conditions. Additionally, the results suggest that specific weight percentages of ZnO nanoparticles can enhance the performance of transformer oil, contributing to more reliable transformer operation."
    },
    {
        "pii": "S0957417425024704",
        "title": "Multi-phase Transformer for remote sensing image super-resolution",
        "abstract": "Remote sensing images have become a critical instrument in diverse applications such as environmental monitoring. Moreover, remote sensing images exhibit a critical phenomenon: the recurrence of similar visual patterns across distant locations. However, these images frequently suffer from low-resolution limitations due to hardware constraints and adverse imaging conditions. Current Transformer-based methods have achieved significant advancements in remote sensing super-resolution tasks, attributed to their capability to capture long-range dependencies and aggregate features globally. However, the current Transformers face two critical challenges in high-resolution image restoration: (1) Redundant token representations due to the irrelevance of most tokens. (2) Inefficient global feature extraction with limited reduced computational effort. To address these limitations, we propose the Multi-phase Transformer (MPT) for remote sensing image super-resolution, which adaptively mitigates irrelevant token interference and effectively extracts spatial features with long-range dependencies under low computational overhead. Specifically, the Sparse Self-Attention (S-SA) in the Sparse Transformer block (STB) utilizes the top-k sparse operation to identify crucial tokens. This mechanism retains vital inter-image feature dependencies while ensuring computational efficiency, thereby resolving the first challenge. Then, the Dense Self-Attention (D-SA) in the Dense Transformer block (DTB) utilizes the Iterative Aggregation Module (IAM) to aggregate input features into fixed maps. This design captures spatial context and transfers global information via cross-attention while maintaining minimal computational complexity, effectively solving the second challenge. To improve the representation of multi-scale features in Multi-phase Transformer blocks, the Multi-dimensional Feature Fusion Module (MFFM) investigates multi-scale correlations within input feature maps and facilitates enhanced cross-resolution interactions. Extensive experiments illustrate that our MPT surpasses current cutting-edge methods, including CNN and Transformer architectures, in both qualitative and quantitative evaluations. In short, MPT surpasses the cutting-edge CAT method by 0.19 dB in average PSNR while requiring only 60.13 % computational resources and 67.38 % parameters."
    },
    {
        "pii": "S1361841525002105",
        "title": "Hierarchical Vision Transformers for prostate biopsy grading: Towards bridging the generalization gap",
        "abstract": "Practical deployment of Vision Transformers in computational pathology has largely been constrained by the sheer size of whole-slide images. Transformers faced a similar limitation when applied to long documents, and Hierarchical Transformers were introduced to circumvent it. This work explores the capabilities of Hierarchical Vision Transformers for prostate cancer grading in WSIs and presents a novel technique to combine attention scores smartly across hierarchical transformers. Our best-performing model matches state-of-the-art algorithms with a 0.916 quadratic kappa on the Prostate cANcer graDe Assessment (PANDA) test set. It exhibits superior generalization capacities when evaluated in more diverse clinical settings, achieving a quadratic kappa of 0.877, outperforming existing solutions. These results demonstrate our approach’s robustness and practical applicability, paving the way for its broader adoption in computational pathology and possibly other medical imaging tasks. Our code is publicly available at https://github.com/computationalpathologygroup/hvit."
    },
    {
        "pii": "S2590123025018912",
        "title": "A proposed wavelet analysis based fault diagnosis scheme of power transformers using fault signatures and CT saturation",
        "abstract": "Diagnosis of concealed internal faults within power transformer is a key for high grid reliability to ensure continuity of power supply to customers. One of the urgent situations of power transformer is the faults under CT saturation and the operation under inrush currents that lead to huge failure of fault identification of the power transformer. In this paper, a fault identification scheme is designed using details and approximate coefficients obtained by discreet wavelet transform applied to a differential current signal under different situations. Also, this paper considers the impact of transformer internal faults such as turn to earth and turn to turn faults, external faults, and inrush currents. The signature of processing differential current is employed for identifying these fault conditions since such fault has a distinct differential current signature. The simulation tests are performed on a 115/22 kV power transformer using ATP-EMTP real-time simulator. Different wavelet families are assessed to show that the optimum mother wavelet, db1, has high fault detection and classification performance. The proposed scheme is verified for transformer energization conditions, and the influence of CT saturation is also considered in this study. Moreover, one of the most important proposed scheme features is simplicity with high lights aspects toward all fault conditions and fault types at different fault location and different fault resistances. Intensive simulation results are obtained to prove the improved selectivity and sensitivity of the proposed scheme for identifying internal transformer faults. Furthermore, sensitivity analysis is not only conducted in terms of transformer loading and fault resistance variation, but transformer scalability study is also verified. Finally, to evaluate the performance of the proposed scheme, an assessment study is adopted to show the accuracy and reliability of differential protection scheme."
    },
    {
        "pii": "S0168169925008518",
        "title": "KDI-Transformer: A method for identifying kiwifruit leaf disease severity in complex environments",
        "abstract": "The accurate identification of the severity of kiwifruit leaf diseases faces significant challenges due to the high morphological similarity between different disease states and interference from complex environmental factors. To address this issue, we propose a Vision Transformer-based severity grading model for kiwifruit leaf diseases, called KDI-Transformer. This model deeply integrates the global modeling capability of Transformer with the local feature extraction advantages of Convolutional Neural Networks (CNNs). It incorporates three innovative modules: the Multi-Scale Perception Module (MSP), which extracts multi-granularity lesion features using parallel multi-scale convolutional kernels and integrates contextual information at different scales; the Adaptive Feature Transmission Module (AFT), which uses dynamic gating weights to adaptively adjust the inter-layer feature transmission ratio, effectively alleviating the feature attenuation problem in deep networks; and the Local-Global Interaction Module (LGI), which employs an attention mechanism for dynamic calibration of local features under global semantic guidance, significantly enhancing the model’s sensitivity to subtle disease differences. Experimental results demonstrate that KDI-Transformer achieves an accuracy of 89.57 %, significantly outperforming various baseline models, and provides a new solution for precise crop management in smart agriculture."
    },
    {
        "pii": "S0952197625017130",
        "title": "Frequency-prompt guided spectral–spatial transformer for hyperspectral image classification",
        "abstract": "Convolutional neural networks (CNNs) and vision transformers (ViTs) have been applied to hyperspectral image (HSI) classification. However, CNNs have difficulty in capturing global context information, while ViTs lack the excellent ability of CNNs to capture local relationships in images. How to integrate CNNs and ViTs to design an effective network remains a challenge. In addition, most of the HSI classification methods are built upon the original image domain, failing to utilize the prior knowledge of the frequency domain. Although some studies explore the frequency-domain information, they cannot effectively achieve spatial-spectral-frequency information interactions and deep feature fusion. To solve the above challenges, we propose a frequency-prompt guided spectral–spatial transformer (FPGSST) for HSI classification. The backbone of FPGSST adopts a two-branch hierarchical network structure, where each branch extracts local and global spatial/spectral feature based on multi-stage convolutions and transformers. To guide the network to learn discriminative spatial and spectral features, we carefully design spatial-wise and spectral-wise frequency prompt modules. These prompts are structural which contain low-frequency priors and learnable high-frequency components. By embedding them into the self-attention mechanism, our model can focus on frequency-domain information in both spatial and spectral domains. We also design a spectral–spatial fusion transformer and a multi-stage fusion transformer to integrate comprehensive features. The resulting FPGSST achieves local–global information interaction and multi-stage spectral–spatial-frequency feature fusion by flexibly harnessing the variants of transformer architecture. Experimental results on several datasets demonstrate the superiority of FPGSST over the compared methods for HSI classification."
    },
    {
        "pii": "S0893608025005763",
        "title": "Burst denoising transformer with multi-task optical flow estimation",
        "abstract": "Burst denoising focuses on producing a clean image from a series of noisy frames captured in rapid succession. A major challenge during burst capturing is the misalignment between frames, caused by subtle movements of the camera or the scene. To deal with this difficulty, in this paper we introduce a novel Burst Denoising Transformer (BDFormer) network. First, we introduce a Transformer-based Multi-task Optical Flow Estimation module (TMOFE) to align the frames, where an auxiliary denoising task is used to reduce the impact of noise during optical flow estimation. Next, the aligned frames are passed through a Transformer-based Feature Enrichment module (TFE). The core unit of TFE lies in a specially-designed Spatial and Channel-wise Transformer Block (SCTB), which combines an FFT-based Spatial Transformer Block (FSTB) and a Channel-wise Transformer Block (CTB), in order to fully leverage both spatial and channel-wise global information across inter- and intra-frames. Extensive experiments show that BDFormer outperforms other transformer-based methods, achieving superior performance while maintaining low computational complexity."
    },
    {
        "pii": "S3050475925004798",
        "title": "Advancing Data-Driven Turbulence Modeling with Transformer Neural Networks: A Comprehensive Review",
        "abstract": "Turbulence modeling remains a fundamental challenge in fluid dynamics due to the complex, chaotic, and multi-scale nature of turbulent flows. Recently, transformer neural networks have emerged as powerful tools capable of capturing the intricate spatiotemporal dependencies inherent in turbulence. This review comprehensively examines the current state-of-the-art applications of transformer neural networks in turbulence modeling. We identify key trends, including the adaptation of transformer architectures with specialized attention mechanisms, the integration of hybrid models combining transformers with convolutional or graph neural networks, and the incorporation of physical constraints to enhance model fidelity. Comparative analyses reveal that transformer-based models often achieve higher accuracy than other artificial intelligence approaches and computational efficiency than traditional computational fluid dynamics methods, excelling in capturing complex flow phenomena and generalizing across different flow conditions and geometries. However, challenges persist, such as high computational demands, data scarcity, difficulties in capturing small-scale turbulent structures, and error accumulation in long-term predictions. Our critical evaluation highlights the strengths of transformer architectures, such as their ability to model global interactions through self-attention mechanisms and handle multidimensional spatiotemporal data, while also discussing weaknesses like computational inefficiency and interpretability issues. By addressing these limitations through architectural innovations, efficient training strategies, incorporation of physical constraints, and leveraging hybrid models, transformer neural networks hold significant promise in advancing turbulence modeling. We conclude that their continued development and integration into fluid dynamics research have the potential to revolutionize the field, leading to more accurate, efficient, and reliable simulations that can benefit a wide range of scientific and engineering applications."
    },
    {
        "pii": "S2590174525002351",
        "title": "Thermal and electromagnetic analysis of a 200 kVA transformer with heat pipe cooling",
        "abstract": "Integrating copper wicked heat pipes (CWHPs) is a proven method to improve oil immersed power transformer (OIP/transformer) heat dissipation. However, determining the most effective placement and number of the heat pipes, and addressing electrical and magnetic constraints associated with the heat pipes integration remains a challenge. This study combines numerical modeling of a 200 kVA OIP/transformer with experimental analysis of a CWHP’s performance to identify the most suitable number and arrangement of CWHPs. Results show that the replacement of cooling fins with 58 CWHPs – even before optimizing their placements − decreases hot-spot temperature (HS/temperature) by 9.8 °C. Besides thermal performance, numerical modeling was conducted to assess the potential electrical and magnetic impacts of the CWHPs, demonstrating almost no negative effects on the OIP/transformer in this sense. However, small eddy currents induced in CWHPs by magnetic field fluctuations generate a very tiny extra heat loss of 0.799 W, an insignificant figure compared to the total 2986 W dissipated heat by active components. Generally, using 28 CWHPs located distant from active components provides optimum temperature reduction in HS/temperature by 14.3%. This will significantly enhance the insulation life of windings as well as the reliability of the OIP/transformer. The results of this article demonstrate the practicability and effectiveness of utilizing CWHPs as a thermal management system for OIP/transformers."
    },
    {
        "pii": "S1674927825001145",
        "title": "Life cycle CO2 emissions of various transformers under different scenarios with multiple functional units",
        "abstract": "Despite extensive research on transformer life cycle carbon emissions, the selection of an appropriate functional unit remains challenging. Many commonly used functional units inadequately reflect transformer functionalities, hindering accurate comparisons across different voltage levels and capacities. This study performs a life cycle carbon emission analysis to evaluate the impact of different functional unit settings on ten transformers. The average emission per studied transformer is 223.47 t CO2eq. Carbon emission varies remarkably based on the choice of the functional unit, with values of 6955.40 kg CO2eq/MVA, 9.83 kg CO2eq/(MVA kV), 81.08 kg CO2eq/(kW h) and 1.36 × 10−4 kg CO2eq/(MW h kV). Metals are identified as the primary contributors to the overall carbon footprint of transformers, accounting for approximately 73.91% of total emissions, with steel contributing up to 42.98%. Selecting greener raw materials, employing recycled materials and using clean energy in production can help reduce transformer carbon emissions. Scenario analysis reveals that improvements in the electricity generation mix reduce emissions during transformer production by 9%–11%, depending on the level of improvement, with larger improvements demonstrating greater reduction rates. The findings of this study provide insights for selecting appropriate functional units for the carbon assessments of transformers and offer a reference for formulating targeted emission reduction strategies to support the power sector's low-carbon transition."
    },
    {
        "pii": "S0262885625002549",
        "title": "Advancing heart disease diagnosis with vision-based transformer architectures applied to ECG imagery",
        "abstract": "Cardiovascular disease, a critical medical condition that affects the heart and blood vessels, requires timely detection for effective clinical intervention. This includes coronary artery disease, heart failure, and myocardial infarction. Our goal is to improve the detection of heart disease through proactive interventions and personalized treatments. Early identification of at-risk individuals using advanced technologies can mitigate disease progression and reduce adverse outcomes. Using recent technological advancements, we propose a novel approach for heart disease detection using vision transformer models, namely Google-Vit, Microsoft-Beit, Deit, and Swin-Tiny. This marks the initial application of transformer models to image-based electrocardiogram (ECG) data for the detection of heart disease. The experimental results demonstrate the efficacy of vision transformers in this domain, with BEiT achieving the highest classification accuracy of 95.9% in a 5-fold cross-validation setting, further improving to 96.6% using an 80-20 holdout method. Swin-Tiny also exhibited strong performance with an accuracy of 95.2%, while Google-ViT and DeiT achieved 94.3% and 94.9%, respectively, outperforming many traditional models in ECG-based diagnostics. These findings highlight the potential of vision transformer models in enhancing diagnostic accuracy and risk stratification. The results further underscore the importance of model selection in optimizing performance, with BEiT emerging as the most promising candidate. This study contributes to the growing body of research on transformer-based medical diagnostics and paves the way for future investigations into their clinical applicability and generalizability."
    },
    {
        "pii": "S1389128625004001",
        "title": "Transformers model for DDoS attack detection: A survey",
        "abstract": "Distributed Denial of Service (DDoS) attack detection through Transformer models is one of the innovative Deep Learning applications. DDoS attacks are hard to handle and there is no definitive solution. Therefore, detecting DDoS attacks based on the Transformer architecture are being widely explored because of its versatility and customization. Transformer architectures analyze network traffic and identify malicious patterns, given different advantages from these architectures, such as the processing capacity in long sequences, the attention mechanism (a.k.a., self-attention) aimed at capturing complex patterns in the identification of malicious traffic, real-time detection through parallelism, the generalization to new types of attacks and, finally, the complete integration with other artificial intelligence techniques. Therefore, this survey is an extensive literature review providing an overview of the Transformer Architecture through different applied models, strategies for data preprocessing, and applications in various types of data, including real-time, address different machine learning techniques and deep learning. Thus, it analyzed 45 papers that focus on detecting DDoS attacks. The F1-Score of the DDoS attack detection identified in the papers varies between 47.40% and 100.00%. This survey contributes to the understanding of relevant aspects in different models applied in transformer architecture and thus emphasizes open issues and research directions."
    },
    {
        "pii": "S0893608025006690",
        "title": "Combining aggregated attention and transformer architecture for accurate and efficient performance of Spiking Neural Networks",
        "abstract": "Spiking Neural Networks (SNNs), which simulate the spiking behavior of biological neurons, have attracted significant attention in recent years due to their distinctive low-power characteristics. Meanwhile, Transformer models, known for their powerful self-attention mechanisms and parallel processing capabilities, have demonstrated exceptional performance across various domains, including natural language processing and computer vision. Despite the significant advantages of both SNNs and Transformers, directly combining the low-power benefits of SNNs with the high performance of Transformers remains challenging. Specifically, while the sparse computing mode of SNNs contributes to reduced energy consumption, traditional attention mechanisms depend on dense matrix computations and complex softmax operations. This reliance poses significant challenges for effective execution in low-power scenarios. Traditional methods often struggle to maintain or enhance model performance while striving to reduce energy consumption. Given the tremendous success of Transformers in deep learning, it is a necessary step to explore the integration of SNNs and Transformers to harness the strengths of both. In this paper, we propose a novel model architecture, Spike Aggregation Transformer (SAFormer), that integrates the low-power characteristics of SNNs with the high-performance advantages of Transformer models. The core contribution of SAFormer lies in the design of the Spike Aggregated Self-Attention (SASA) mechanism, which significantly simplifies the computation process by calculating attention weights using only the spike matrices query and key, thereby effectively reducing energy consumption. Additionally, we introduce a Depthwise Convolution Module (DWC) to enhance the feature extraction capabilities, further improving overall accuracy. We evaluated SAFormer on the CIFAR-10, CIFAR-100, Tiny-ImageNet, DVS128-Gesture, and CIFAR10-DVS datasets and demonstrated that SAFormer outperforms state-of-the-art SNNs in both accuracy and energy consumption, highlighting its significant advantages in low-power and high-performance computing."
    },
    {
        "pii": "S1566253525005895",
        "title": "Foundation models and Transformers for anomaly detection: A survey",
        "abstract": "In line with the development of deep learning, this survey examines the transformative role of Transformers and foundation models in advancing visual anomaly detection (VAD). We explore how these architectures, with their global receptive fields and adaptability, address challenges such as long-range dependency modeling, contextual modeling and data scarcity. The survey categorizes VAD methods into reconstruction-based, feature-based and zero/few-shot approaches, highlighting the paradigm shift brought about by foundation models. By integrating attention mechanisms and leveraging large-scale pre-training, Transformers and foundation models enable more robust, interpretable, and scalable anomaly detection solutions. This work provides a comprehensive review of state-of-the-art techniques, their strengths, limitations, and emerging trends in leveraging these architectures for VAD."
    },
    {
        "pii": "S0263224125017269",
        "title": "Online identification method for inter-turn short-circuit fault of grounding transformer winding and phase distinction",
        "abstract": "The grounding transformer in a substation’s medium-voltage ungrounded system establishes a neutral point and also functions as a station transformer. Its winding configuration and fault characteristics are different from those of conventional distribution transformers. Moreover, inter-turn short-circuit faults in grounding transformer windings are common; however, effective online monitoring methods are still lacking. To address this issue, this study examines the current variation characteristics of inter-turn short-circuit faults in grounding transformers under various operating conditions through theoretical analysis, simulations, and case studies. An online identification method is proposed to detect inter-turn short-circuit faults in grounding transformer windings and to distinguish affected phases based on current variation characteristics. First, an equivalent circuit model is developed for theoretical analysis of the current variation characteristics associated with inter-turn short-circuit faults in different phase windings. The theoretical findings are then validated using finite element simulations. Subsequently, the impact of single-phase grounding faults on high-voltage side currents is analyzed using theoretical formulations and actual fault recording data. Additionally, the influence of low-voltage winding load variations on current characteristics is examined using simulations. Finally, an online identification framework is developed to detect inter-turn short-circuit faults in different grounding transformer phases. The results demonstrate that the proposed method effectively identifies inter-turn short-circuit faults in high-voltage main windings, phase-shifting windings, and low-voltage windings in real-time."
    },
    {
        "pii": "S0378779625005619",
        "title": "Short-term multi-step wind power prediction model based on Pt-Transformer neural network integrating spatio-temporal feature and sparse attention",
        "abstract": "The accuracy and stability of wind power prediction are crucial for grid dispatching. However, precise wind power prediction faces three major challenges: effective mitigating data noise, representing complex spatio-temporal features, and selecting an appropriate prediction model. To address these challenges, this paper proposes a new wind power prediction model called process temporal transformer (Pt-Transformer). Firstly, an exponential weighted moving average (EWMA) method is employed to filter the noise of original wind characteristics while maintaining data complexity. Secondly, a seasonal trend decomposition (STD) model combined with a temporal autoencoder (TAE) is implemented to efficiently extract complex spatio-temporal characteristics. Furthermore, a sparse attention mechanism combined with the Transformer architecture is introduced to effectively extract high-dimensional latent space representations. Finally, two case studies are conducted using data from a cluster of wind farms and a single wind farm in central China, and three models, namely Transformer, Prob-Transformer, and Pt-Transformer, are used for prediction. The results show that the Pt-Transformer model outperforms the other two models in prediction accuracy and stability, reflecting its excellent performance in short-term multi-step wind power generation prediction."
    },
    {
        "pii": "S0952197625011157",
        "title": "Transformers in pathological image analysis: A survey",
        "abstract": "With the advancements of artificial intelligence, deep learning has emerged as the predominant approach in computational pathology. It is dedicated to automatically analyzing the intricate phenotype information embedded in various pathological images, with the goal of delivering more precise diagnoses, prognoses, and treatment recommendations for cancer patients. As the latest breakthrough in deep learning technology, Transformers are gaining traction in the realm of pathological image analysis by harnessing self-attention mechanisms to capture global information. Consequently, this study presents a comprehensive review of state-of-the-art models leveraging Transformers, applied across tasks such as classification, segmentation, and survival analysis in pathological image analysis. Initially, we delineate the concept and key components of Transformers, followed by a survey of their recent applications in pathology. These applications encompass pathological image classification, segmentation, lesion detection and localization, as well as the utilization of specific Transformer architectures for patient survival analysis. Subsequently, we delve into the challenges encountered in employing Transformers for pathological image analysis and speculate on future developmental trajectories. Our aim is to furnish readers with an exhaustive roadmap to deepen their comprehension of Transformer applications in pathology, thereby fostering the advancement of more sophisticated technologies and enabling more precise diagnoses and treatment strategies for cancer patients."
    },
    {
        "pii": "S2213138825002632",
        "title": "Ultra-short sequence-augmented power system transient stability assessment using transformer-based deep learning",
        "abstract": "The increasing penetration of renewable energy into power grids has intensified concerns regarding transient stability. Traditional transient stability assessment methodologies, which rely on time domain simulations and energy function-based approaches, face significant limitations in computational efficiency and real-time applicability. To address these challenges, this paper proposes a novel dual-phase framework that integrates Long Short-Term Memory (LSTM) networks and a modified Transformer architecture for enhanced transient stability evaluation. In the first phase, an LSTM-based trajectory reconstruction module is employed to extrapolate complete 5-second dynamic trajectories from initial measurements of 0.5 s post-fault. This addresses the critical issue of temporal data insufficiency during the early stages of system perturbations, enabling accurate representation of electromechanical transients. The second phase employs a Transformer classifier, which processes the reconstructed trajectories to assess system stability. The Transformer’s architecture incorporates tailored positional encoding to align with the frequency and timescale characteristics of system dynamics, while its self-attention mechanism facilitates the extraction of global temporal patterns essential for stability classification. A case study on the IEEE 39-bus system demonstrates the efficacy of the proposed framework. Within the critical post-fault assessment window, the method achieves an accuracy of 99.6%, significantly outperforming conventional methods in both computational efficiency and classification accuracy."
    },
    {
        "pii": "S2666546825000679",
        "title": "Transformer-based forecasting for high-frequency natural gas production data",
        "abstract": "Accurate prediction of natural gas well production data is crucial for effective resource management and innovation, particularly amid the global transition to sustainable energy. Traditional models struggle with high-frequency, high-dimensional datasets generated by digital transformation in the oil and gas industry. This study explores the application of Transformer-based models — Transformer, Informer, Autoformer, and Patch Time Series Transformer (PatchTST) — for forecasting high-frequency natural gas production data. These models utilize self-attention mechanisms to capture long-term dependencies and efficiently process large-scale datasets. Autoformer achieves predictive success through its Seasonal Decomposition Attention mechanism, which effectively extracts trend-seasonality patterns. However, our experiments show that Autoformer exhibits sensitivity to dataset changes, as performance declines when using old parameters compared to retrained models, highlighting its reliance on dataset-specific retraining. Experimental results demonstrate that increasing sampling frequency significantly enhances prediction accuracy, reducing MAPE from 0.556 to 0.239. Additionally, these models consistently track actual production trends across extended forecast horizons. Notably, PatchTST maintains stable performance using either pretrained or retrained parameters, showcasing superior adaptability and generalization. This makes it particularly suitable for real-world applications where frequent retraining may not be feasible. Overall, the findings validate the applicability of Transformer-based models, particularly PatchTST, in dynamic and precise natural gas production forecasting. This study provides valuable insights for advancing adaptive, data-driven resource management strategies."
    },
    {
        "pii": "S0010482525010777",
        "title": "KS-TMIL: A K-Stage Transformer approach with multiple instance learning model for ovarian cancer subtype classification",
        "abstract": "Existing multiple instance learning (MIL) methods treat whole slide image (WSI) as collections of independent patches, neglecting these crucial spatial and morphological connections. This paper proposes K-Stage Transformer with multiple instance learning framework (KS-TMIL) that leverages the power of transformers. Unlike traditional MIL methods, KS-TMIL excels at modeling relationships between patches within a WSI. This capability is achieved through a multi-stage transformer that incorporates inception based position encoding generator (IBPEG) block. This enables KS-TMIL to effectively leverage the weakly supervised labels and achieve accurate classifications. KS-TMIL offers a family of models with varying numbers of transformer layers, catering to different classification tasks. The evaluation is conducted using publicly available Kaggle dataset “UBC ovarian cancer subtype classification and outlier detection”, demonstrating the effectiveness of the proposed model with an AUC and balanced accuracy of 99% and 93.3% respectively."
    },
    {
        "pii": "S2772941925001504",
        "title": "Research and application of intelligent learning path optimization based on LSTM-Transformer model",
        "abstract": "In the global wave of digital learning, how to optimize personalized learning paths and improve learning efficiency has become a key issue to be solved urgently in the field of education. Based on this, this study proposes a hypothesis: the intelligent learning path optimization strategy based on the LSTM-Transformer model can achieve accurate prediction and personalized optimization of learners' learning paths with the help of deep learning technology. In the research process, the LSTM model was used to capture the characteristics of the learner's learning behavior sequence, and the self-attention mechanism of the Transformer model was introduced to deepen the understanding of the learner's learning status. Experimental comparison shows that compared with the traditional learning path recommendation algorithm, the optimization strategy based on the LSTM-Transformer model has achieved remarkable results, with the learner's knowledge mastery rate greatly increased from 75 % to 95 %, the learning time shortened by about 25 %, and the learning satisfaction also increased from 70 % to 90 %, which verifies the research hypothesis and fully proves that the LSTM-Transformer model has high application value in intelligent learning path optimization."
    },
    {
        "pii": "S0263224125015817",
        "title": "Integrating Machine Learning Models for Enhancing Power Transformer Health Assessment Based on the Insulating Paper State",
        "abstract": "Effective health assessment of power transformers is essential for maintaining reliable power systems, with insulation deterioration often indicating potential faults. Traditional Dissolved Gas Analysis (DGA) methods, while common, have limitations in providing precise diagnostics for transformer insulation health. This paper presents an advanced approach using machine learning (ML) models and non-invasive diagnostic parameters, enhanced by data balancing techniques, to improve transformer health assessments. Key parameters like dissolved gases, moisture, and acidity were used to train models such as Decision Trees, K-Nearest Neighbors, and Artificial Neural Networks, with performance optimized through systematic hyperparameter tuning. The results demonstrate notable accuracy gains, highlighting ML’s advantage over traditional methods. Parameter adjustments reduce noise sensitivity and overfitting, boosting predictive stability. This paper illustrates the potential of ML-driven, non-invasive techniques for transformer monitoring, offering a scalable solution that enhances predictive maintenance, minimizes risks, and supports longer asset lifespans and improved system reliability. This paper presents a machine learning-based non-invasive approach to assess power transformer health by predicting insulating paper condition. By integrating 21 diagnostic parameters from electrical, thermal, and chemical tests with advanced data balancing techniques, our framework achieved perfect classification (100% accuracy and F1-score) using Decision Tree and Bagging classifiers, with LightGBM delivering near-perfect performance (97.9% accuracy, 0.98 F1-score). This represents a significant improvement over traditional dissolved gas analysis methods, while requiring no internal inspection. The proposed methodology successfully classified paper conditions across degradation states with remarkable precision, providing utilities with a reliable tool for maintenance prioritization."
    },
    {
        "pii": "S0263876225003405",
        "title": "Self-attention transformer architectures for cyberattack detection and secure state reconstruction of integrated chemical process systems",
        "abstract": "This study presents a centralized self-attention-based transformer architecture for detecting and identifying adversarial cyberattacks in highly integrated chemical process systems, with subsequent integration into post-attack state estimation and recovery. Inherent inter-process dependencies in such systems complicate sensor-targeted attack isolation, leading to measurement errors and closed-loop instability. Two transformer-based frameworks are developed: (1) an end-to-end decoder-only transformer for simultaneous attack detection and sensor attribution, and (2) a modular two-tier architecture using a feedforward neural network for detection and a transformer for sensor identification. Both models are trained using closed-loop simulations of the benzene alkylation benchmark process, regulated by integrated model predictive control (MPC) and moving horizon estimation (MHE). These simulations are used to generate a comprehensive dataset that includes single- and multi-sensor attacks across four distinct attack categories, enabling robust model generalization across diverse transient regimes and attack intensities. The detection–identification modules are integrated with an extended Kalman filter (EKF) for corrupted state reconstruction. Results demonstrate high detection accuracy and improved sensor classification in the two-tier model, while EKF integration enhances system resilience and stability under attack conditions."
    },
    {
        "pii": "S2214157X25009128",
        "title": "Thermal performance optimization of 10 kV dry-type transformer: A dual-strategy approach combining silicone rubber-enhanced insulation material and air gaps redesign",
        "abstract": "The traditional dry-type transformers with epoxy resin as insulation material suffer from poor heat dissipation efficiency, which may cause overheating faults. Comparatively, silicone rubber has excellent heat and electronic insulation performance. Due to its wide application in the power industry, silicone rubber has the potential to overcome this limitation, but it has been rarely discussed in previous studies. In this work, two main research themes are studied. The first is proposing a heat dissipation optimization by the substitution of high-performance silicone rubber for epoxy resin (EP). The second is the adjustment of air gap dimensions, by adopting improved non-dominated sorting genetic algorithm II (NSGA-II) to perform multi-objective optimization. The electromagnetic-thermal-fluid coupling simulation is conducted to obtain the results, whose accuracy has been validated by experiments. Compared to the original dry-type transformer, the hotspot temperature of the transformer has been reduced by 8.43 °C through the insulation material replacement. Moreover, multi-objective optimization generated 95 sets of Pareto-optimal solutions, one of the Pareto solutions selected shows that the hotspot temperature is reduced by 12.71 °C totally, and the temperature distribution overall has been improved as well. These results revealed the significant role of the dual-strategy solution in improving heat dissipation performance of dry-type transformers."
    },
    {
        "pii": "S1068520025001798",
        "title": "Prediction of freeze-thaw damage of asphalt concrete based on distributed fiber optic sensors and KAN-Transformer fusion model",
        "abstract": "Asphalt mixtures are widely used in road construction but are vulnerable to damage caused by freeze–thaw cycles. This study introduces a novel approach to real-time monitoring using distributed fiber optic sensing (DFOS) technology and proposes an innovative deep learning fusion network architecture combining Kolmogorov-Arnold Network (KAN) and Transformer models. Real-time monitoring of asphalt beams during freeze–thaw cycles is achieved through DFOS, which collects data on strain, temperature, and other critical physical parameters. The KAN-Transformer model, along with Transformer-based time series models, is employed for data processing and feature extraction to detect and predict minor changes in material properties during freeze–thaw cycles. The results demonstrate that the KAN-Transformer model outperforms the traditional Transformer model, which suffers from limited parallel processing capability and sensitivity to hyperparameter tuning, leading to improved accuracy in predicting freeze–thaw damage evolution. This study not only validates the superior damage prediction accuracy of the KAN-Transformer model but also offers an efficient method for field applications in asphalt concrete damage prediction."
    },
    {
        "pii": "S2468227625003059",
        "title": "Transformer-Based Models for Sentiment Analysis of YouTube Video Comments",
        "abstract": "The rapid expansion of online video content has positioned platforms like YouTube as substantial sources of multimedia data, rich in diverse user interactions and expressions. Among these, comments have emerged as a significant source of sentiment, opinion, and feedback. The objective of this research is to improve comprehension of the emotions conveyed in YouTube comments through the assessment and comparison of transformer-based sentiment analysis algorithms. The main objectives were to assess various transformer architectures, including BERT, GPT, RoBERTa, and T5, for their effectiveness in sentiment classification tasks. The research utilized the Just Dance dataset for training and evaluated the models on several state-of-the-art sentiment analysis datasets to ensure robustness and generalizability. Methods involved exploring different pre-training tech- niques, loss functions, optimizers, and hyperparameter settings to optimize model performance. The findings reveal that RoBERTa consistently demonstrated superior performance, achieving the highest accuracy and F1-scores across various configurations, particularly with its specialized to- kenizer and the AdamW optimizer. This study contributes novel insights into the strengths and limitations of transformer-based approaches for analyzing YouTube comments and offers valu- able recommendations for improving sentiment analysis frameworks across diverse contexts while suggesting avenues for future improvements."
    },
    {
        "pii": "S1574013725000425",
        "title": "Attention-based transformer models for image captioning across languages: An in-depth survey and evaluation",
        "abstract": "Image captioning involves generating textual descriptions from input images, bridging the gap between computer vision and natural language processing. Recent advancements in transformer-based models have significantly improved caption generation by leveraging attention mechanisms for better scene understanding. While various surveys have explored deep learning-based approaches for image captioning, few have comprehensively analyzed attention-based transformer models across multiple languages. This survey reviews attention-based image captioning models, categorizing them into transformer-based, deep learning-based, and hybrid approaches. It explores benchmark datasets, discusses evaluation metrics such as BLEU, METEOR, CIDEr, and ROUGE, and highlights challenges in multilingual captioning. Additionally, this paper identifies key limitations in current models, including semantic inconsistencies, data scarcity in non-English languages, and limitations in reasoning ability. Finally, we outline future research directions, such as multimodal learning, real-time applications in AI-powered assistants, healthcare, and forensic analysis. This survey serves as a comprehensive reference for researchers aiming to advance the field of attention-based image captioning."
    },
    {
        "pii": "S1566253525004208",
        "title": "Transformers and large language models for efficient intrusion detection systems: A comprehensive survey",
        "abstract": "With significant advancements in Transformers and large language models (LLMs), natural language processing (NLP) has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols. This survey paper provides a comprehensive analysis of the utilization of Transformers and LLMs in cyber-threat detection systems. The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research. The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field. The survey explores the application of Transformers in intrusion detection systems (IDSs), focusing on different architectures such as Attention-based models, LLMs like BERT and GPT, CNN/LSTM-Transformer hybrids, and emerging approaches like Vision Transformers (ViTs), and more. Furthermore, it explores the diverse environments and applications where Transformers and LLMs-based IDS have been implemented, including computer networks, Internet of things (IoT) devices, critical infrastructure protection, cloud computing, software-defined networking (SDN), as well as in autonomous vehicles (AVs). The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more. Finally, the conclusion summarizes the findings and highlights the significance of Transformers and LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development."
    },
    {
        "pii": "S1047320325001427",
        "title": "FUT: Frequency-aware U-shaped transformer for image denoising",
        "abstract": "Transformer has a powerful ability in capturing global dependencies of all pixels or regions with the help of self-attention mechanism and achieves remarkable denoising performance. However, such achievements are obtained at the cost of deeply stacked networks and repeated displacements to adjust the feature maps. To address these issues, this paper develops a frequency-aware U-shaped transformer (FUT) for image denoising, which consists of the encoding and decoding procedures. In the encoding process, there are two modules including down-sampling and convolution residual blocks, where down-sampling block explores a multi-spectral attention mechanism to extract different spectral information between different channels and preserve their richness. The convolution residual block uses a spatial attention mechanism that automatically captures important regional features, remaining robust to operations such as cropping, translation, and rotation. In the decoding procedure, a dual-branch transformer is employed to reduce the depth of the network, in which a feature map pixel exchange method is reported to adjust the whole feature map without multiple displacements. Meanwhile, a dual-branch up-sampling for preserving both global and local information is presented. Ablation experiments about the selection of different modules in our FUT are conducted to validate their effectiveness and extensive experimental results in both quantitation and qualification demonstrate that our FUT method can achieve competitive denoising performance, and even outperforms the SOTA denoising methods."
    },
    {
        "pii": "S2667295225000406",
        "title": "KANs-DETR: Enhancing Detection Transformer with Kolmogorov–Arnold Networks for small object",
        "abstract": "This research proposed an end-to-end object detection network based on Kolmogorov–Arnold Networks (KANs)-Detection Transformer (DETR). KANs block was introduced into encoder–decoder structure instead of the full connection layer to dynamically learn the activation function and improve the robustness and accuracy of the model. Experiments showed that the detection capability of KANs-DETR on multicategory object detection was better than that of HGNetv2 and Swin Transformer as backbone. Furthermore, in order to solve the problem of insensitivity to small objects, the Squeeze-and-Excitation module was applied for feature fusion and presented better performance. The KANs-DETR achieved high detection accuracy and efficiency in handling small objects in complex scenes, providing a new perspective for network optimization."
    },
    {
        "pii": "S2590123025009648",
        "title": "Optimal parameter extraction of equivalent circuits for single- and three- phase Power transformers based on arctic puffin algorithm accomplished with experimental verification",
        "abstract": "The power transformer is a critical device in power systems. This paper addresses one of the major problems which hopes to enhance the accuracy of estimation of parameters, which is critical in power transformer modeling, maintenance, and operating efficiency. In that context, this work estimates the parameters of single- and three-phase power transformers by a new optimizer called Arctic Puffin Optimization Algorithm (APO). The algorithm is intended to improve estimation of transformer parameters with the goal of reducing the error incurred between the estimated and actual values of the parameters. To verify the accuracy of the APO, experimental measurements were conducted on single- and three-phase transformers. The assessment of the algorithm's effectiveness was performed against the effectiveness of other commonly used estimating methods. The results have shown that APO increases the accuracy of estimation of the parameters of both single- and three-phase transformers to considerable levels. Dependability of the APO was established by experimental verification, which disclosed an ultimate connection between the resultant quantities and actual measurements. The study also confirmed APO can be useful for transformer parameter estimation because APO converges more rapidly and more precisely compared with traditional methods of the literature."
    },
    {
        "pii": "S2214157X25009037",
        "title": "Thermal behavior and switching losses of MOSFET in high-frequency transformer circuits",
        "abstract": "The rapid development of semiconductor technology has increased attention to heat generation in electronic components, particularly in Metal-Oxide-Semiconductor Field-Effect Transistors (MOSFETs), which are key elements in modern power circuits. MOSFETs are critical elements in transformer circuits, and under high-frequency switching conditions, they require completely thermal analysis to ensure reliability and efficiency. This study establishes a robust experimental measurement system and introduces a reliable methodology to simultaneously measure temperature variations and current in MOSFETs. In the experiments, MOSFETs are driven by Pulse-Width Modulation (PWM) signals to perform switching operations, and a PT1000 temperature probe securely attached to the MOSFET surface provides precise temperature readings. The observed temperature changes are directly proportional to Joule heating, as determined from the measured currents. However, using a transformer load versus a resistive load leads to distinct thermal and electrical behaviors. Specifically, in transformer-based circuits, the MOSFET temperature variation exhibits pronounced frequency-dependent characteristics. With a nonlinear transformer load, the MOSFETs switching duration is extended, leading to significant heat generation due to the increased drain-source resistance during switching, which consequently results in increased switching losses. To address this, we propose an energy-conservation-based method to estimate the MOSFETs effective resistance during the switching process. This method not only improves understanding of heat dissipation in MOSFETs but also provides valuable insights for optimizing high-frequency power circuit designs."
    },
    {
        "pii": "S0957417425022511",
        "title": "Reinforcement learning based early classification framework for power transformer differential protection",
        "abstract": "The balance between response speed and diagnosis accuracy forms a critical concern in transformer protection. However, prevailing AI-based transformer protection methods tend to adopt fixed data length to extract electrical quantity information, thus impeding prompt responsiveness to situations where discriminative fault features emerge in the early stages. This study formulates transformer protection as a Markov decision process and proposes an Early Classification Proximal Policy Optimization (ECPPO) framework to utilize reinforcement learning (RL) for data-length adaptive transformer protection with timely action and notable high accuracy. However, the limited generalization of RL algorithms poses a significant issue in the transformer protection scenario. While enhancing the feature extraction capability of a model is essential for improving its generalization ability, ECPPO constructs a two-stage training paradigm to augment the policy model accordingly. In the first stage, a multi-task deep learning framework trains a feature-extraction module with normalization layers employing fault label information and a signal reconstruction task to enrich the feature representation. In the second stage, the pre-trained feature-extraction module is transferred to the agent model with frozen weights, and PPO training is performed. Additionally, to improve the utilization efficiency of samples, a period-circle-shift data augmentation method is proposed, which enhances the generalization capability by cyclically reconstructing data in periodic sequences. To validate the proposed framework, a series of experiments were conducted using simulation data generated by PSCAD/EMTDC software as the training data and practical data generated by experimental transformer system as the testing data. The experimental results demonstrate a significantly enhanced testing accuracy of 99.19 %, coupled with an average response time of 12.10 ms, indicating that the ECPPO algorithm not only achieves superior accuracy but also effectively reduces the average response time. Furthermore, the results highlight its robust generalization capability when transitioning from simulation to experimental systems."
    },
    {
        "pii": "S1474034625004082",
        "title": "Bayesian cooperative probabilistic Transformer for remaining useful life prediction with uncertainty estimation in industrial equipment",
        "abstract": "Remaining useful life (RUL) prediction is a key task in prognostics and health management (PHM) of industrial equipment, crucial for enabling reliable predictive maintenance and enhancing equipment reliability. Transformer is widely used for RUL prediction due to its ability to capture remote dynamics and nonlinear responses. However, mainstream Transformers focus on overall prediction accuracy and lack the capability to effectively assess uncertainty of RUL prediction. Furthermore, equal treatment of multi-source information or incorporating additional multi-source information fusion modules during Transformer training can increase prediction uncertainty. To address these challenges, a novel Bayesian cooperative probabilistic Transformer method for machine RUL prediction is proposed, which deeply combines the Bayesian network with the Transformer to comprehensively estimate prediction uncertainty. Prediction uncertainty in Transformer consists of attentional uncertainty, cognitive uncertainty, and aleatoric uncertainty. The core idea is that a negative log-likelihood loss function is first designed to capture attentional uncertainty, while cognitive and aleatoric uncertainties are quantified using Bayesian approach and noise channel. Additionally, an uncertainty-guided multi-source fusion strategy dynamically integrates low and high-uncertainty models for accurate RUL prediction. The performance and advantages of this method are verified by experiments on the C-MAPSS dataset and the nuclear circulating water pump (NCWP) dataset."
    },
    {
        "pii": "S0141938225001623",
        "title": "Dual attention and cross physical-guided Transformers for simultaneous enhancement and super-resolution of underwater imagery",
        "abstract": "Underwater images are severely degraded due to the scattering and absorption of light underwater, often resulting in low resolution, particularly when captured with low bitrates and hardware limitations. Additionally, the use of auxiliary light sources in dim underwater conditions leads to non-uniform illumination (NUI) in underwater images. To address these challenges, a novel method based on dual attention and cross physical-guided Transformers for simultaneous enhancement and super-resolution of underwater imagery (DAPT-SESR) is proposed. Firstly, to address the inconsistent attenuation of underwater light, a multi-head covariance attention (MCA) mechanism is proposed for the compensation of channel information. Moreover, a dual attention Transformers block (DATB) is proposed to fully extract both channel and spatial features, structured sequentially to connect spatial and channel Transformers. Subsequently, to address both over-enhancement and under-enhancement in NUI underwater images, the physical model of underwater imaging is considered. Additionally, a novel and versatile cross physical-guided Transformer block (CPTB) is proposed to effectively extract physical information from the transmission map. Finally, experimental results demonstrate that the proposed DAPT-SESR outperforms current methods in terms of both quantitative and qualitative evaluations."
    },
    {
        "pii": "S1574013725000449",
        "title": "Transformers in speech processing: Overcoming challenges and paving the future",
        "abstract": "The remarkable success of transformers in the field of natural language processing has sparked interest in their potential for mod- elling long-range dependencies within speech sequences. Transformers have gained prominence across various speech-related do- mains, including automatic speech recognition, speech synthesis, speech translation, speech para-linguistics, speech enhancement, spoken dialogue systems, and numerous multimodal applications. However, the integration of transformers in speech processing comes with significant challenges such as managing the high computational costs, handling the complexity of speech variability, and addressing the data scarcity for certain speech tasks. In this paper, we present a comprehensive survey that aims to bridge research studies from diverse subfields within speech technology. By consolidating findings from across the speech technology landscape, we provide a valuable resource for researchers interested in harnessing the power of transformers to advance the field. We identify the challenges encountered by transformers in speech processing while also offering insights into potential solutions to address these issues."
    },
    {
        "pii": "S2214157X25007841",
        "title": "Analytical and numerical evaluation of an oil–water heat exchanger applied on power transformer considering the mineral/vegetable insulating oil replacement",
        "abstract": "Oil–water plate heat exchangers are used in the cooling systems of power transformers to maintain internal temperatures below specific limits, ensuring proper and safe operation. This paper presents an analytical and numerical evaluation of an oil–water heat exchanger applied to power transformers. An analytical thermal-hydraulic model that solves for pressure drop, fluid flow, and temperature distributions along the channels between the plates is implemented. After being validated using data from a heat exchanger manufacturer, demonstrating good accuracy, the model is applied to analyze the feasibility of replacing the insulating fluid from mineral oil to vegetable oil and its impact on the internal temperatures of the power transformer. Modifying the insulating fluid while retaining the original constructive characteristics can be detrimental to the power transformer. Therefore, aspects such as the variation in the number of plates and the pumped oil flow rate are also evaluated."
    },
    {
        "pii": "S1746809425007888",
        "title": "TUTNet: Leveraging transformers for comprehensive extraction and preservation of global information in medical image segmentation",
        "abstract": "In the field of medical image segmentation, the introduction of Transformers to assist in medical image segmentation has proven to be effective in recent years. Compared to traditional CNN methods, Transformers have excellent global context information extraction capabilities. However, using a purely Transformer-based architecture for image segmentation not only increases the overall network parameters but also decreases the ability to extract local features. The accurate extraction and integration of local and global features are crucial for achieving medical image segmentation. Therefore, we propose TUTNet, an improved medical image segmentation network. TUTNet features a dual encoder architecture, consisting of a CNN-based encoder and a Transformer-based encoder. The former effectively extracts local information, while the latter captures global information. Skip connections simultaneously receive information from both encoders, eliminating semantic differences between the two encoders through a cross-attention mechanism. Skip connections are also of high importance for medical image segmentation. In our network, the skip connections utilize a purely Transformer mechanism, fully leveraging the information extracted by encoders at all levels, utilizing a self-attention mechanism to focus on channel information, and then employing a cross-attention mechanism to query and filter global information with local information, thus extracting valuable information for decoders at each level. We conducted extensive validation on four datasets, demonstrating the effectiveness of our network across different feature datasets. Our work can be viewed on https://github.com/whycantChinese/TUTNet."
    },
    {
        "pii": "S0925231225011695",
        "title": "Simplified Transformer",
        "abstract": "Based on two interesting mathematical observations that almost all square matrices are diagonalizable and invertible, this paper proposes a simplified transformer called SimplTrans, which eliminates parameters in the attention mechanism. It is known that the success of the Transformer can be attributed to its embedded attention mechanism, which plays a crucial role in the network. However, the introduction of a large number of parameters with this attention mechanism hinders the efficiency of learning and computation in the Transformer. The proposed SimplTrans leverages this parameter-free attention mechanism, leading to a significant reduction in the number of parameters. As a result, the proposed SimplTrans achieves improved learning and computation efficiency compared to the original Transformer. Furthermore, the complexity of SimplTrans is also reduced to that of the original Transformer, resulting in superior performance across various tasks, as demonstrated in our experiments. The associated code can be found at https://github.com/IntelligentInterdisciplinary/SimplTrans."
    },
    {
        "pii": "S0168169925008361",
        "title": "TDR-Transformer: A transformer neural network model to determine soil relative permittivity variations along a time domain reflectometry sensor waveguide",
        "abstract": "Interpreting soil relative permittivity ( ε r ) variations along a time domain reflectometry (TDR) waveguide provides an opportunity to determine soil water content at multiple depths using a vertically installed TDR sensor. Compared to placing sensors at different depths, vertical sensor installation reduces measurement efforts and enhances data-use-efficiency. Revealing ε r variations includes two aspects: identifying ε r change positions and determining ε r values. Traditional inverse analyses are not widely applied due to their high computational demands. Machine learning-based methods, e.g., TDR-CNN, provide a forward computational workflow to track ε r change positions and reduce computational load, but errors in ε r values are relatively large. In this study, TDR-Transformer is developed as a new waveform interpretation model to improve ε r estimation accuracy. Modified from the standard transformer architecture, an encoder with convolutional neural layers is used to extract waveform geometric features, and a decoder generates a sequence of ε r values to represent ε r variations. Attention is a mechanism that can dynamically extract and process the relevant information within the data, which processes and integrates the waveform geometric information in the encoder, ensures the causality (time-order) of the waveform data in the decoder, and transfers information from the encoder to the decoder. TDR-Transformer was trained and tested using simulated waveforms where ε r changes along the waveguides, but soil electrical conductivity (EC) was assumed to be small and stable. The RMSE for ε r values was within 0.5–1.6 % and the RMSE of ε r change positions was within 5–8 %. A soil infiltration experiment and a precipitation-evaporation experiment illustrated applications of TDR-Transformer to observed waveforms. Consequently, TDR-Transformer is a promising artificial intelligence model to interpret TDR waveforms in soils with nonuniform ε r , and fine-tuning TDR-Transformer is recommended for specific commercial TDR sensor designs."
    },
    {
        "pii": "S1270963825005668",
        "title": "Near-Earth asteroid detection using video transformer networks",
        "abstract": "This paper presents a novel deep learning approach to detecting faint Near-Earth Asteroids (NEAs) with a high apparent motion. The approach utilizes video transformer networks, which unlike conventional single-exposure techniques, can exploit the spatio-temporal relationships that exist within a tracklet, i.e., a collection of consecutive exposures of the same field of view. The proposed detection algorithm consists of two stages: A video transformer classifier first detects the presence of an asteroid, and then transfers positive detections to a video transformer regressor that determines the pixel locations of the asteroid in the tracklet frames. An analysis on the false positive rejection capabilities demonstrates how the detection pipeline avoids common false positives by exploiting the correlated nature of asteroid motion across the tracklet frames. The detection limits in terms of both apparent magnitude and apparent motion are quantified. Computational validations show that for NEA discovery the proposed approach is more efficient computationally than the state-of-the-art synthetic tracking methods."
    },
    {
        "pii": "S0952197625016550",
        "title": "Physics-informed cross layer temporal frequency transformer network for remaining useful life prediction of rolling bearings",
        "abstract": "In response to the challenge of accurately capturing long-term degradation dependencies in the remaining useful life (RUL) prediction task for rolling bearings under extreme operating conditions, which leads to low accuracy of prediction results, a physics-informed cross-layer temporal frequency Transformer method for rolling bearing RUL prediction, named WeiCLTF, has been proposed. Firstly, a cross-layer information transfer mechanism is established between adjacent layers within the Transformer, which introduces cross-layer regularization to facilitate alternate information sharing between adjacent Transformer layers, reducing the discrepancy between the key matrix of the multi-head attention module and the query matrix of the preceding layer. The current layer's feed-forward module weights are then utilized as the weights for the subsequent lower layer, ensuring that the higher layers of the Transformer can accurately refine the information from the lower layers, thus enhancing the network's ability to capture long-term dependencies. Secondly, The self-attention layers in the first two layers of the Transformer are replaced with Fourier modules. This alternation between the encoder and decoder enables the model to capture time-frequency information from the alternating Fourier transform and inverse Fourier transform, thereby enhancing the accuracy of predictions. Finally, use the Weibull failure model to describe the failure probability of the bearing over its lifecycle, and integrate it into the loss function of the neural network training to enhance the physical consistency of the training. The proposed model was applied and verified using both public datasets and datasets of wind turbine gearboxes from real wind farms to confirm its superiority."
    },
    {
        "pii": "S0893608025005945",
        "title": "Decoupling upper and lower face transformers for binary interactive video generation",
        "abstract": "Current audio-driven binary interaction methods have limitations in capturing the uncertain relationship between a speaker’s audio and an interlocutor’s facial movements. To address this issue, we propose a video generation pipeline based on a cross-modal Transformer. First, a Transformer decoder partitions facial features into upper and lower regions, capturing lower features that are closely linked to the audio and upper features that remain independent of visual cues. Second, we design a cross-modal attention module that combines alignment bias with causal attention mechanisms to effectively manage subtle motion variations between adjacent frames in facial sequences. To mitigate uncertainties in long-term contexts, we expand the self-attention range of the Transformer encoder and integrate self-supervised pretrained speech representations to alleviate data scarcity. Finally, by optimizing the audio-to-action mapping and incorporating an enhanced neural renderer, we achieve fine control over facial movements while generating high-quality portrait images. Extensive experiments validate the effectiveness and superiority of our approach in interactive video generation."
    },
    {
        "pii": "S1350449525003123",
        "title": "Hyperspectral image classification based on multi-scale hierarchical convolution and spectral-spatial Transformer",
        "abstract": "Hyperspectral image (HSI) classification is one of the study hotspots in geoscience and remote sensing image intelligent processing tasks. In the past few years, the combination of convolutional neural network (CNN) and Transformer has become a mainstream for HSI classification task. Although such methods have achieved some success but still face some challenges. When conducting local information modelling, CNN ignores the intrinsic variability of HSI multi-dimensional features, which limits the ability of network to extract the texture and structural information of ground objects. When modelling the long-range dependency relationships by Transformer, there are deficiencies in the utilization of the spectral and spatial coupling correlations of HSI. To this end, a HSI classification method based on multi-scale hierarchical convolution and spectral-spatial Transformer (HC-SST) is proposed in this paper. First, the multi-scale spatial and spectral features of HSI are gradually extracted through multi-scale hierarchical convolution to capture the local detail information under different receptive fields. Second, the local semantic marker is utilized to perform advanced semantic modelling on the shallow features, enhancing the expression ability of the local features. Then, the spectral Transformer and the spatial Transformer model the global spatial and spectral features separately to avoid the confusion of different dimensional information. Finally, the presented adaptive feature fusion module promotes the deep interaction of the global spatial and spectral features, and realizes the effective fusion of information of different dimensions. Experiments on three public HSI datasets and a self-made HSI dataset show that the presented method achieves superior classification results and possesses stronger generalization ability compared with some state-of-the-art methods."
    }
]